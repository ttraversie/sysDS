\documentclass{article}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{ dsfont }
\usepackage{amsmath}
\usepackage{filemod}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{todonotes}

\newcommand{\MilestoneOneGlobalDeviationEq}{4~}
\newcommand{\MilestoneOneBaselineEq}{5~}
\newcommand{\MilestoneOneComputingTime}{3.1.5~}

% If you use BibTeX in apalike style, activate the following line:
\bibliographystyle{acm}

\title{CS-449 Project Milestone 1: Personalized Recommender with k-NN}

\author{
\textbf{Name}: Clément Francis\\
\textbf{Sciper}: xxx\\
\textbf{Email:} clement.francis@epfl.ch\\
\textbf{Name}: Thomas Traversié\\
\textbf{Sciper}: 350372\\
\textbf{Email:} thomas.traversie@epfl.ch\\
}

\begin{document}
\maketitle

\section{Motivation: Movie Recommender}
(No Q)
\section{Proxy Problem: Predicting Ratings}
(No Q)

\section{Baseline: Prediction based on Global Average Deviation}
\subsection{Questions}
\label{section:q1}

Implement the previous prediction methods using Scala's standard library, without using Spark.

\begin{equation}
    \label{eq:baseline}
    p_{u,i} = \bar r_{u,\bullet} + \bar{\hat r}_{\bullet,i} * scale( (\bar r_{u,\bullet} + \bar{\hat r}_{\bullet,i}), \bar r_{u,\bullet})
\end{equation}
  

\begin{itemize}
  % python stats.py data/ml-100k/u.data
  
    \item[\textbf{B.1}] \textit{Compute and output the global average rating ($\bar r_{\bullet,\bullet}$), the average rating for user 1 ($\bar r_{1,\bullet}$),  the average rating for item 1 ($\bar r_{\bullet,1}$), the average deviation for item 1 ($\bar{\hat r}_{\bullet,1}$), and the predicted rating of user 1 for item 1 ($p_{1,1}$, Eq~\ref{eq:baseline}) using \texttt{data/ml-100k/u2.base} for training. When computing the item average for items that do not have ratings in the training set, use the global average ($\bar r_{\bullet, \bullet}$). When making predictions for items that are not in the training set, use the user average if present, otherwise the global average.}
    
    In order to compute the global average rating, we write the function \texttt{meanRatings}, which takes a sequence of ratings Seq[Rating] and returns a double. We first filter the sequence, we map each Rating to its value and then we apply the function \texttt{mean}.
    
    The computation of the average rating of a given user is done through the function \texttt{meanRatingUser}, which takes as arguments a user and a sequence of ratings and returns a double. We filter the sequence to keep the ratings corresponding to the user and we compute the mean.
    
    We compute similarly the average rating of a given item with the fuunction \texttt{meanRatingItem} but we filter on the user.
    
    We compute the average deviation of an item with the function \texttt{meanNormalizedItem}. We use the intermediary functions \texttt{scale} and \texttt{normalize}, which compute the scale and the normalized deviation. 
    
    The predicted ating is computed with \texttt{prediction}, applying the given formula and using the functions already defined. If the user has no rating in the training set, the average rating of the user will be 0.0 (due to the definition of \texttt{mean}), thus we return the global average.
    
    We find : $\bar r_{\bullet,\bullet}$ = 3.5265, $\bar r_{1,\bullet}$ = 3.6330, $\bar r_{\bullet,1}$ = 3.8883, $\bar{\hat r}_{\bullet,1}$ = 0.3027 and $p_{1,1}$ = 4.0468.
    
    
  
    \item [\textbf{B.2}] \textit{Compute the prediction accuracy (average MAE on \texttt{ml-100k/u2.test}) of the previous methods ($\bar r_{\bullet, \bullet}$, $\bar r_{u,\bullet}$, $\bar r_{\bullet,i}$) and that of the proposed baseline ($p_{u,i}$, Eq.~\ref{eq:baseline}). }
    
    We write four functions that take a training set and return a predictor. \texttt{predictionMean}, \texttt{predictionUser}, \texttt{predictionItem} and \texttt{predictionBaseline}. To write efficient functions, we use objects of type Map to store for each user to its average rating, for each item to its average rating and its average deviation. This method avoid the computation of same data multiple times. 
    
    We write the function \texttt{mae} which compute the MAE for a given predictor and a given testing set. We find the following MAE: 0.9489 for $\bar r_{\bullet, \bullet}$, 0.8383 for $\bar r_{u,\bullet}$, 0.8207 for $\bar r_{\bullet,i}$ and 0.7607 for the proposed baseline $p_{u,i}$

  \item [\textbf{B.3}] \textit{Measure the time required for computing the MAE for all ratings in the test set (\texttt{ml-100k/u2.test}) with all four methods by recording the current time before and after (ex: with \texttt{System.nanoTime()} in Scala). The duration is the difference between the two. } 
 
 \textit{ 
Include the time for computing all values required to obtain the answer from the input dataset provided in the template: recompute from scratch all necessary values even if they are available after computing previous results (ex: global average $\bar r_{\bullet, \bullet}$). Also ensure you store the results in some auxiliary data structure (ex: $\texttt{Seq[(mae, timing)]}$) as you are performing measurements to ensure the compiler won't optimize away the computations that would otherwise be unnecessary.}

\textit{
 For all four methods, perform three measurements and compute the average and standard-deviation.}
 
 \textit{In your report, show in a figure the relationship between prediction precision (MAE) on the x axis, and the computation time on the y axis including the standard-deviation. Report also the technical specifications (model, CPU speed, RAM, OS, Scala language version, and JVM version) of the machine on which you ran the tests. Which of the four prediction methods is the most expensive to compute? Is the relationship between MAE and computation linear? What do you conclude on the computing needs of more accurate prediction methods?}
 
 The Baseline method is the more expensive one to compute. The results are what we expected: the more sophisticated the method is, the more computations and time it needs. The relationship between the MAE value and the computation time is not linear: it is increasingly expensive to compute more accurately. Therefore, more accurate prediction methods requires more computing needs.
 
 The computer we use has the following technical specifications: model: HP ENVY x360, CPU speed: 2.10 GHz, RAM: 8 Go, OS: Microsoft Windows 10, Scala language version: 2.11.12,  JVM version: 1.8.0\_321.

\end{itemize}

\section{Spark Distribution Overhead}

\subsection{Questions}
\label{section:q5}

Implement $p_{u,i}$ using Spark RDDs. Your distributed implementation should give the same results as your previous implementation using Scala's standard library. Once your implementation works well with \texttt{data/ml-100k/u2.base} and \texttt{data/ml-100k/u2.test}, stress test its performance with the bigger \newline \texttt{data/ml-25m/r2.train} and \texttt{data/ml-25m/r2.test}. 

\begin{itemize}
  
   \item [\textbf{D.1}] \textit{Ensure the results of your distributed implementation are consistent with \textbf{B.1} and \textbf{B.2} on  \texttt{data/ml-100k/u2.train} and \texttt{data/ml-100k/u2.test}. Compute and output the global average rating ($\bar r_{\bullet,\bullet}$), the average rating for user 1 ($\bar r_{1,\bullet}$),  the average rating for item 1 ($\bar r_{\bullet,1}$), the average deviation for item 1 ($\bar{\hat r}_{\bullet,1}$), and the predicted rating of user 1 for item 1 ($p_{1,1}$, Eq~\ref{eq:baseline}). Compute the prediction accuracy (average MAE on \texttt{ml-100k/u2.test}) of the proposed baseline ($p_{u,i}$, Eq.~\ref{eq:baseline}). } 
  
    \item [\textbf{D.2}] \textit{Measure the combined time to (1) pre-compute the required baseline values for predictions and (2) to predict all values of the test set on the 25M dataset, \texttt{data/ml-25m/r2.train} and \texttt{data/ml-25m/r2.test}. Compare the time required by your implementation using Scala's standard library (\textbf{B.1} and \textbf{B.2}) on your machine, and your new distributed implementation using Spark on \texttt{iccluster028}. Use 1 and 4 executors for Spark and repeat all three experiments (predict.Baseline, distributed.Baseline 1 worker, distributed.Baseline 4 workers) 3 times. Write in your report the average and standard deviation for all three experiments, as well as the specification of the machine on which you ran the tests (similar to B.3).}
    
    \textit{As a comparison, our reference implementation runs in 44s on the cluster with 4 workers. Ensure you obtain results roughly in the same ballpark or faster. Don't worry if your code is slower during some executions because the cluster is busy.}
    
    \textit{Try optimizing your local Scala implementation by avoiding temporary objects, instead preferring the use of mutable collations and data structures. Can you make it faster, running locally on your machine without Spark, than on the cluster with 4 workers? Explain the changes you have made to make your code faster in your report.}
  
\end{itemize}

\section{\textit{Personalized} Predictions}

\subsection{Questions}
\label{section:q1}

\begin{equation}
    \label{eq:similarity}
    %s_{u,v} = \frac{\sum_{r_{u,i},r_{v,i} \in \text{Train}} \hat r_{u,i} * \hat r_{v,i}}
    %                     {\sum_{r_{u,i},r_{v,i} \in \text{Train}} | \hat r_{u,i}| * |\hat r_{v,i}|}
    s_{u,v} = \begin{cases}
                   \frac{\sum_{i \in (I(u) \cap I(v))} \hat r_{u,i} * \hat r_{v,i}}
                         { \sqrt{\sum_{i \in I(u)} {(\hat r_{u,i})}^{2}} * \sqrt{\sum_{i \in I(v)} {(\hat r_{v,i})}^{2}}} &
                                    (I(u) \cup I(v)) \neq \emptyset;
                                \exists_{i \in I(u)} \hat r_{u,i} \neq 0; 
                                 \exists_{i \in I(v)} \hat r_{v,i} \neq 0 \\
                         0 & \text{otherwise}
                    \end{cases}
\end{equation}


\begin{equation}
    \label{eq:personalized-prediction}
    p_{u,i} = \bar r_{u,\bullet} + \bar{\hat r}_{\bullet,i}(u) * scale( (\bar r_{u,\bullet} + \bar{\hat r}_{\bullet,i}(u)), \bar r_{u,\bullet})
\end{equation}

  
\begin{itemize}
    \item [\textbf{P.1}] \textit{Using uniform similarities of 1 between all users, compute the predicted rating of user 1 for item 1 ($p_{1,1}$) and the prediction accuracy (MAE on \texttt{ml-100k/u2.test}) of the personalized baseline predictor.} 
    
    To compute a personalized predictor for a specified similarity, we write a unique function \texttt{predictionPersonalized}, the similarity used being notified by an argument. For the predictor with uniform similarity, the results are obviously the same than the baseline predictor: $p_{1,1}$ = 4.0468 and MAE = 0.7604.
    
    \item [\textbf{P.2}] \textit{Using the the adjusted cosine similarity (Eq.~\ref{eq:similarity}), compute the similarity between user $1$ and user $2$ ($s_{1,2}$), the predicted rating of user 1 for item 1 ($p_{1,1}$ Eq.~\ref{eq:personalized-prediction}) and the prediction accuracy (MAE on \texttt{ml-100k/u2.test}) of the personalized baseline predictor.} 
    
    In order to compute the Cosine similarity between two users with the function \texttt{simCosine}, we use a Map that associates each user to : (1) the list of the items rated by the user and (2) an other map which associates the items rated by the user to their ratings. We write \texttt{ratingsPreProcessed} to compute the preprocessed ratings.
    
    We obtain: $s_{1,2}$ = 0.0730, $p_{1,1}$ = 4.0870 and MAE =  0.7372. The personalized prediction with cosine similarity is thus more accurate than the four other methods.
    
        \item [\textbf{P.3}] \textit{Implement the Jaccard Coefficient\footnote{\url{https://en.wikipedia.org/wiki/Jaccard_index}}. Provide the mathematical formulation of your similarity metric in your report. User the jaccard similarity, compute the similarity between user $1$ and user $2$ ($s_{1,2}$), the predicted rating of user 1 for item 1 ($p_{1,1}$ Eq.~\ref{eq:personalized-prediction}) and the prediction accuracy (MAE on \texttt{ml-100k/u2.test}) of the personalized baseline predictor. Is the Jaccard Coefficient better or worst than Adjusted Cosine similarity?}
        
        The Jaccard coefficient between users $u$ and $v$ is computed thanks to the formula $$ s_{u,v} = \frac{\mid I(u) \cap I(v)\mid}{\mid I(u) \cup I(v)\mid}$$
        
        We obtain: $s_{1,2}$ = 0.0309, $p_{1,1}$ = 4.0940 and MAE = 0.7560. The Jaccard coefficient is worst than Adjusted Cosine similarity. This is logical: two users may have seen the same movies, but we do not have indication on whether they have rated the movies similarly or not.
        
        
\end{itemize}

\section{Neighbourhood-Based Predictions}


\subsection{Questions}
\label{section:q2}

\begin{itemize}    
        \item [\textbf{N.1}] \textit{Implement the k-NN predictor. Do not include self-similarity in the k-nearest neighbours. Using $k=10$,  \texttt{data/ml-100k/u2.base} for training output the similarities between: (1) user $1$ and itself; (2) user $1$ and user $864$; (3) user $1$ and user $886$. Still using $k=10$, output the prediction for user 1 and item 1 ($p_{1,1}$), and make sure that you obtain an MAE of $0.8287 \pm 0.0001$ on \texttt{data/ml-100k/u2.test}.} 
        
        We need to create a new similarity for the k-nearest neighbours predictions, based on the cosine similarity: if $v$ is among the $k$ nearest neighbours of $u$, the the similarity between $u$ and $v$ is their cosine similarity; if not, the similarity is 0. To compute the k-nearest neighbours similarities efficiently, we create a Map that associates a user $u$ to the sequence that contains ($v$, $s_{u,v}$) (with $s_{u,v}$ the adjusted cosine similaity) for the k nearest neighbours $v$ of $u$. This is done by \texttt{kNNusers}. To compute the similarity between $u$ and $v$, we just check whereas $v$ is in the value of the key $u$ in the map, and return the value if it is, 0 otherwise.
        
        We write \texttt{ratingItemkNN} to compute the user-specific weighted-sum deviation for an item $i$ and a user $u$ using the new similarity. We are then able to write the function \texttt{predictionKNN}, following the model of the previous predictors.
        
        With $k=10$, we find that $s_{1,1}=$ 0 (which is logic because we exclude self-similarity), $s_{1,864}=$ 0.2423, $s_{1,886}=$ 0 and $p_{1,1}$ =  4.3191.
    
    \item [\textbf{N.2}] \textit{Report the MAE on \texttt{data/ml-100k/u2.test} for $k = {10, 30, 50, 100, 200, 300, 400, 800, 942}$. What is the lowest $k$ such that the MAE is lower than for the baseline (non-personalized) method?} 
    \begin{center}
    \begin{tabular}{|c|c|}
  \hline
  $k$ & $MAE$ \\
  \hline
  10 & 0.8287 \\
  \hline
  30 & 0.7810 \\
  \hline
  50 & 0.7618 \\
  \hline
  100 & 0.7467 \\
  \hline
  200 & 0.7401 \\
  \hline
  300 & 0.7392 \\
  \hline
  400 & 0.7391 \\
  \hline
  800 & 0.7392 \\
  \hline
  943 & 0.7372 \\
  \hline
	\end{tabular}
	\end{center}
	
	We observe that the accuracy of the prediction increases with the $k$. The MAE seems to have a logarithmic shape: after $k=$ 50, the improvement of the prediction is smaller.
	
	With the baseline (non-personalized) method, we have found a MAE of 0.7607. Thus the lowest $k$ such that the MAE of the k-nearest neighbours is loweer than the MAE of the baseline prediction is $k=$ 100. 
	
	The results are coherent: if we take all the neignbours into consideration, the k-nearest neighbours is equivalent to the personalized prediction with cosine similarity.
    
     \item [\textbf{N.3}] \label{q-total-time} \textit{Measure the time required for computing predictions (without using Spark) on \texttt{data/ml-100k/u2.test}. Include the time to train the predictor on \newline \texttt{data/ml-100k/u2.base} including computing the similarities $s_{u,v}$ and using $k=300$. Try reducing the computation time with alternative implementation techniques (making sure you keep obtaining the same results). Mention in your report which alternatives you tried,  which ones were fastest, and by how much. The teams with the correct answer and shortest times on a secret test set will obtain more points on this question.}
    
    With the same computer as for the previous measurements, the time of computation is 58 seconds. 
    
    We have already  implemented a way to minimize the number of computations (with the map).     
     
\end{itemize}


\section{Recommendation}

\subsection{Questions}
\label{section:q4}

\begin{itemize} 
   \item [\textbf{R.1}] \textit{Train a k-NN predictor with training data from \texttt{data/ml-100k/u.data}, augmented with additional ratings from user "$944$" provided in \texttt{personal.csv}, using adjusted cosine similarity and $k=300$. Report the prediction for user 1 item 1 ($p_{1,1}$)}.
   
   We concatenate the two sequences of data (\texttt{data} and \texttt{personnal}) and we train \texttt{predictionKNN} on this concatenation. We find $p_{1,1}$ = 4.1322.

  \item [\textbf{R.2}] \textit{Report the top 3 recommendations for user "$944$" using the same k-NN predictor as for \textbf{R.1}.  Include the movie identifier, the movie title, and the prediction score in the output. If additional recommendations have the same predicted value as the top 3 recommendations, prioritize the movies with the smallest identifiers in your top 3 (ex: if the top 8 recommendations all have predicted scores of \texttt{5.0}, choose the top 3 with the smallest ids.) so your results do not depend on the initial permutation of the recommendations.}
  
  We create the function recommendation, which takes as arguments a user, an integer $n$, a sequence of Ratings objects and a predictor, and returns the top $n$ movies that the user has not seen, with the predicted rating. The recommendations are sorted by predicted scores, and in case of equality we favour the smallest ids. To implement this, we proceed to two sort: first we sort by id and then by score, thus in case of equality we have the expected result.
  
  For the previous predictor and user 944, the top 3 recommendations are: [119, "Maya Lin: A Strong Clear Vision (1994)", 5], [814, "Great Day in Harlem", 5], [1189, "Prefontaine (1997)", 5].
  
\end{itemize}


\end{document}
